{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "\n",
    "    position_1=Input((774,))\n",
    "    position_2=Input((774,))\n",
    "    \n",
    "    model_encode=Sequential()\n",
    "    \n",
    "    model_encode.add(Dense(units=700, input_dim=774, use_bias=False,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=None)))\n",
    "    model_encode.add(BatchNormalization())\n",
    "    model_encode.add(Activation(\"relu\"))\n",
    "    \n",
    "    model_encode.add(Dense(units=500, use_bias=False,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=None)))\n",
    "    model_encode.add(BatchNormalization())\n",
    "    model_encode.add(Activation(\"relu\"))\n",
    "    \n",
    "    model_encode.add(Dense(units=400, use_bias=False,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=None)))\n",
    "    model_encode.add(BatchNormalization())\n",
    "    model_encode.add(Activation(\"relu\"))           \n",
    "              \n",
    "    model_encode.add(Dense(units=300, use_bias=False,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=None)))\n",
    "    model_encode.add(BatchNormalization())\n",
    "    model_encode.add(Activation(\"relu\"))\n",
    "    \n",
    "    encoded_position_1=model_encode(position_1)\n",
    "    encoded_position_2=model_encode(position_2)\n",
    "    \n",
    "    prediction=Concatenate()([encoded_position_1, encoded_position_2])\n",
    "    \n",
    "    prediction=Dense(units=800, use_bias=False,\n",
    "                     kernel_initializer=keras.initializers.he_normal(seed=None))(prediction)\n",
    "    prediction=BatchNormalization()(prediction)\n",
    "    prediction=Activation(\"relu\")(prediction)\n",
    "    \n",
    "    prediction=Dense(units=600, use_bias=False,\n",
    "                     kernel_initializer=keras.initializers.he_normal(seed=None))(prediction)\n",
    "    prediction=BatchNormalization()(prediction)\n",
    "    prediction=Activation(\"relu\")(prediction) \n",
    "    \n",
    "    prediction=Dense(units=400, use_bias=False,\n",
    "                     kernel_initializer=keras.initializers.he_normal(seed=None))(prediction)\n",
    "    prediction=BatchNormalization()(prediction)\n",
    "    prediction=Activation(\"relu\")(prediction)\n",
    "    \n",
    "    prediction=Dense(units=100, use_bias=False,\n",
    "                     kernel_initializer=keras.initializers.he_normal(seed=None))(prediction)\n",
    "    prediction=BatchNormalization()(prediction)\n",
    "    prediction=Activation(\"relu\")(prediction)\n",
    "    \n",
    "    prediction=Dense(units=1, use_bias=False)(prediction)\n",
    "    prediction=BatchNormalization()(prediction)\n",
    "    prediction=Activation(\"sigmoid\")(prediction)\n",
    "    \n",
    "    model=Model(inputs=[position_1, position_2], outputs=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 774)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 774)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 300)          1219400     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 600)          0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 800)          480000      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 800)          3200        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 800)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 600)          480000      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 600)          2400        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 600)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 400)          240000      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 400)          1600        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 400)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          40000       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 100)          400         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 100)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            100         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1)            4           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 1)            0           batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,467,104\n",
      "Trainable params: 2,459,502\n",
      "Non-trainable params: 7,602\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = sparse.load_npz(\"data_test.npz\")\n",
    "X_test_1 = data_test[0:10**5,0:774]\n",
    "X_test_2 = data_test[0:10**5,774:1548]\n",
    "y_test = data_test[0:10**5,1548]\n",
    "del data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1889s 378us/step - loss: 0.4358 - acc: 0.7846 - val_loss: 0.4058 - val_acc: 0.8062\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1878s 376us/step - loss: 0.3693 - acc: 0.8271 - val_loss: 0.3841 - val_acc: 0.8198\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1875s 375us/step - loss: 0.3484 - acc: 0.8390 - val_loss: 0.3690 - val_acc: 0.8287\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1882s 376us/step - loss: 0.3356 - acc: 0.8458 - val_loss: 0.3569 - val_acc: 0.8344\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    data = sparse.load_npz(\"data_\" + str(i) + \".npz\")\n",
    "    X_train_1 = data[:,0:774]\n",
    "    X_train_2 = data[:,774:1548]\n",
    "    y_train = data[:,1548]\n",
    "    del data\n",
    "    model.fit([X_train_1, X_train_2], y_train, epochs=1, batch_size=512, validation_data=([X_test_1, X_test_2], y_test), verbose=1)\n",
    "    model.save(\"model_\" + str(i) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1876s 375us/step - loss: 0.3401 - acc: 0.8436 - val_loss: 0.3404 - val_acc: 0.8439\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1885s 377us/step - loss: 0.3214 - acc: 0.8538 - val_loss: 0.3314 - val_acc: 0.8482\n"
     ]
    }
   ],
   "source": [
    "for i in range(5,7):\n",
    "    data = sparse.load_npz(\"data_\" + str(i) + \".npz\")\n",
    "    X_train_1 = data[:,0:774]\n",
    "    X_train_2 = data[:,774:1548]\n",
    "    y_train = data[:,1548]\n",
    "    del data\n",
    "    model.fit([X_train_1, X_train_2], y_train, epochs=1, batch_size=512, validation_data=([X_test_1, X_test_2], y_test), verbose=1)\n",
    "    model.save(\"model_\" + str(i) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1886s 377us/step - loss: 0.3192 - acc: 0.8549 - val_loss: 0.3340 - val_acc: 0.8473\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1885s 377us/step - loss: 0.3307 - acc: 0.8483 - val_loss: 0.3335 - val_acc: 0.8469\n"
     ]
    }
   ],
   "source": [
    "for i in range(7,9):\n",
    "    data = sparse.load_npz(\"data_\" + str(i) + \".npz\")\n",
    "    X_train_1 = data[:,0:774]\n",
    "    X_train_2 = data[:,774:1548]\n",
    "    y_train = data[:,1548]\n",
    "    del data\n",
    "    model.fit([X_train_1, X_train_2], y_train, epochs=1, batch_size=512, validation_data=([X_test_1, X_test_2], y_test), verbose=1)\n",
    "    model.save(\"model_\" + str(i) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.get_value(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-04"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.get_value(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1880s 376us/step - loss: 0.3091 - acc: 0.8598 - val_loss: 0.3223 - val_acc: 0.8526\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1879s 376us/step - loss: 0.3049 - acc: 0.8619 - val_loss: 0.3195 - val_acc: 0.8539\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1880s 376us/step - loss: 0.3217 - acc: 0.8528 - val_loss: 0.3211 - val_acc: 0.8540\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1879s 376us/step - loss: 0.3124 - acc: 0.8576 - val_loss: 0.3203 - val_acc: 0.8543\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1886s 377us/step - loss: 0.3074 - acc: 0.8603 - val_loss: 0.3200 - val_acc: 0.8549\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1886s 377us/step - loss: 0.3064 - acc: 0.8604 - val_loss: 0.3193 - val_acc: 0.8555\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1882s 376us/step - loss: 0.3057 - acc: 0.8601 - val_loss: 0.3205 - val_acc: 0.8555\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1884s 377us/step - loss: 0.2984 - acc: 0.8640 - val_loss: 0.3203 - val_acc: 0.8557\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1884s 377us/step - loss: 0.3081 - acc: 0.8594 - val_loss: 0.3184 - val_acc: 0.8563\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1890s 378us/step - loss: 0.3162 - acc: 0.8555 - val_loss: 0.3216 - val_acc: 0.8544\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1884s 377us/step - loss: 0.3143 - acc: 0.8566 - val_loss: 0.3178 - val_acc: 0.8555\n",
      "Train on 5000000 samples, validate on 100000 samples\n",
      "Epoch 1/1\n",
      "5000000/5000000 [==============================] - 1884s 377us/step - loss: 0.3086 - acc: 0.8597 - val_loss: 0.3174 - val_acc: 0.8564\n"
     ]
    }
   ],
   "source": [
    "for i in range(9,21):\n",
    "    data = sparse.load_npz(\"data_\" + str(i) + \".npz\")\n",
    "    X_train_1 = data[:,0:774]\n",
    "    X_train_2 = data[:,774:1548]\n",
    "    y_train = data[:,1548]\n",
    "    del data\n",
    "    model.fit([X_train_1, X_train_2], y_train, epochs=1, batch_size=512, validation_data=([X_test_1, X_test_2], y_test), verbose=1)\n",
    "    model.save(\"model_\" + str(i) + \".h5\")"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
